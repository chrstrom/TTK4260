{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: To develop a mathematical understanding of PCA  \n",
    "Principal Components Analysis, also known as PCA, is a technique commonly used for reducing the dimensionality of data while preserving as much as possible of the information contained in the original data. PCA achieves this goal by projecting data onto a lower-dimensional subspace that retains most of the variance among the data points.   \n",
    "(https://programmathically.com/principal-components-analysis-explained-for-dummies/)  \n",
    "\n",
    "Consider this simple two-dimensional graphical explanation of what we aim to do here:  \n",
    "![SpaceRotationGif](../assets/capture_variance_pca.gif)  \n",
    "This exercise attempts to convey mathematically how we achieve the transformation as we see in the animation. If you find any part of the exercise unclear, we highly recommend watching this StatQuest video to get a primer on PCA before solving the exercise: \n",
    "https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([\n",
    "    [0.387, 4878, 5.42],\n",
    "    [0.723, 12104, 5.25],\n",
    "    [1, 12756, 5.52],\n",
    "    [1.524, 6787, 3.94],\n",
    "])\n",
    "X -= np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Iinear Iterative Partial Least-Squares (NIPALS) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to compute PCA using NIPALS algorithm\n",
    "\n",
    "* Step 1: Initialize an arbitrary column vector $\\mathbf{t}_{a}$ either randomly or by just copying any column of X. \n",
    "* Step 2: Take very column of $\\mathbf{X}$, $\\mathbf{X_k}$ and regress it onto the $\\mathbf{t}_{a}$ vector and store the regression coefficeints as $\\mathbf{p}_{ka}$. (Note: This simply means performing an ordinary least squares regression ($y=mx$) with $x=t_{a}$ and $y=X_{k}$ with $m=(\\mathbf{x^\\top}\\mathbf{x})^{-1}\\mathbf{x^\\top}\\mathbf{y}$). In the current notation we get \n",
    "$$p_{ka}=\\frac{\\mathbf{t_a^\\top}\\mathbf{X}_{k}}{\\mathbf{t_a^\\top}\\mathbf{t_a}}$$\n",
    "\n",
    "Repeat it for each of the columns of $X$ to get the entire vector $\\mathbf{p}_{k}$. This is shown in the illustration\n",
    "above where each column from $X$ is regressed, one at a time, on $\\mathbf{t}_{a}$, to calculate the loading entry, $\\mathbf{ùëù}_{ka}$ \n",
    "\n",
    "In practice we don‚Äôt do this one column at time; we can regress all columns in $\\mathbf{X}$ in go: $$\\mathbf{p_a^\\top}=\\frac{1}{\\mathbf{t_a^\\top}\\mathbf{t_a}}.\\mathbf{t_a^\\top}\\mathbf{X_a}$$  where $\\mathbf{t_a}$ is an $N \\times 1$ column vector, and $\\mathbf{X}_{a}$ us an $N \\times K$ matrix.\n",
    "* The loading vector $\\mathbf{p_a^\\top}$ won‚Äôt have unit length (magnitude) yet. So we simply rescale it to have\n",
    "magnitude of 1.0: $$\\mathbf{p_a^\\top}=\\frac{\\mathbf{p_a^\\top}}{\\sqrt{\\mathbf{p_a^\\top}\\mathbf{p_a}}}$$\n",
    "* Step 4: Regress every row in $\\mathbf{X}$ onto this normalized loadings vector. As illustrated below, in our linear regression the rows in $\\mathbf{X}$ are our y-variable each time, while the loadings vector is our x-variable. The regression coefficient becomes the score value for that $ùëñ^{th}$ row:\n",
    "\n",
    "$$p_{i,a}=\\frac{\\mathbf{x}_{i}^{\\top}\\mathbf{p}_{a}}{\\mathbf{p}_{a}^{\\top}\\mathbf{p}_{a}}$$\n",
    "where $x_{i}^{T}$ is a $K \\times 1$ column vector. We can combine these $N$ separate least-squares models and\n",
    "calculate them in one go to get the entire vector, \n",
    "\n",
    "$$\\mathbf{t}_{a}^{\\top}=\\frac{1}{\\mathbf{p}_{a}^{\\top}\\mathbf{p}_{a}}\\mathbf{X}\\mathbf{p}_{a}^{\\top}$$  where $p_{a}$ is a $K \\times 1$ column vector.\n",
    "* Step 5: Continue looping over steps 2,3,4 until the change in vector $t_{a}$ is below a chosen tolerance\n",
    "* Step 6: On convergence, the score vector and the loading vectors, $\\mathbf{t}_{a}$ and $\\mathbf{p}_{a}$ are stored as the $a^{th}$ column in matrix $\\mathbf{T}$ and $\\mathbf{P}$. We then deflate the $\\mathbf{X}$ matrix. This crucial step removes the variability captured in this component ($t_{a}$ and $p_{a}$) from $\\mathbf{X}$:\n",
    "\n",
    "$$E_{a}=X_{a}-t_{a}p_{a}^{\\top}$$\n",
    "\n",
    "$$X_{a+1} = E_{a}$$ \n",
    "\n",
    "For the first component, $X_{a}$ is just the preprocessed raw data. So we can see that the second component is actually calculated on the residuals $E_{1}$, obtained after extracting the first component. This is called deflation, and nicely shows why each component is orthogonal to the others. Each subsequent component is only seeing variation remaining after removing all the others; there is no possibility that two components can explain the same type of variability. After deflation we go back to step 1 and repeat the entire process for the next component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_variance = lambda X: np.sum(np.var(X, axis=0, ddof=1))\n",
    "\n",
    "\n",
    "def PCA(X, n_components, tol=1e-7, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Compute PCA using the NIPALS algorithm.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Training data.\n",
    "        n_components (int): Number of components to keep.\n",
    "        tol (float): Convergence tolerance.\n",
    "        max_iter (int): Maximum iterations before stopping.\n",
    "    Returns:\n",
    "        T (np.ndarray): Scores matrix.\n",
    "        P (np.ndarray): Loadings matrix.\n",
    "        var_explained (np.ndarray): Fraction of variance explained by each PC.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    n_obs, n_vars = X.shape\n",
    "    T = np.zeros((n_obs, n_components))\n",
    "    P = np.zeros((n_vars, n_components))\n",
    "    var_explained = np.zeros(n_vars)\n",
    "    \n",
    "    # Center data and calculate total variance\n",
    "    X_a = X - np.mean(X, axis=0)\n",
    "    total_var = sum_variance(X_a)\n",
    "    current_var = total_var\n",
    "    \n",
    "    for h in range(n_components):\n",
    "        # Choose t_h as any column of X_h\n",
    "        t_h = X_a[:, 0].reshape(-1, 1)\n",
    "        n_r = 0\n",
    "        \n",
    "        finished = False\n",
    "        while not finished:\n",
    "            n_r += 1\n",
    "            # Compute loadings\n",
    "            p_h = np.dot(X_a.T, t_h) / np.dot(t_h.T, t_h)\n",
    "            # Normalize loadings\n",
    "            p_h /= np.linalg.norm(p_h)\n",
    "            # Compute scores\n",
    "            t_h_new = np.dot(X_a, p_h) / np.dot(p_h.T, p_h)\n",
    "            \n",
    "            error = (t_h_new - t_h).T @ (t_h_new - t_h)\n",
    "            t_h = t_h_new\n",
    "            \n",
    "            if error <= tol ** 2:\n",
    "                finished = True\n",
    "            elif n_r >= max_iter:\n",
    "                print(\"Iteration stopped without convergence.\")\n",
    "                finished = True\n",
    "\n",
    "        # Deflate X_a\n",
    "        X_a -= t_h @ p_h.T\n",
    "        \n",
    "        T[:, h] = t_h.flatten()\n",
    "        P[:, h] = p_h.flatten()\n",
    "        \n",
    "        old_var = current_var\n",
    "        current_var = sum_variance(X_a)\n",
    "        var_explained[h] = (old_var - current_var) / total_var\n",
    "        \n",
    "    return T, P, var_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of the NIPALS algorithm\n",
    "* The NIPALS algorithm computes one component at a time. The first component computed is\n",
    "equivalent to the t1 and p1 vectors that would have been found from an eigenvalue or singular value\n",
    "decomposition.\n",
    "* The algorithm can handle missing data in X.\n",
    "* The algorithm always converges, but the convergence can sometimes be slow.\n",
    "* It is also known as the Power algorithm to calculate eigenvectors and eigenvalues.\n",
    "* It works well for very large data sets.\n",
    "* It is used by most software packages, especially those that handle missing data.\n",
    "* Of interest: it is well known that Google used this algorithm for the early versions of their search engine, called PageRank148."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "T, P, pcvar = PCA(X, n_components, max_iter=1e3)\n",
    "\n",
    "print(\"T (Scores)\", T, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"P (Loadings)\", P, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"Variance explained ratio\", np.sqrt(pcvar) / np.sum(np.sqrt(pcvar)), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "U, S, P_T = svd(X, full_matrices=False)\n",
    "Sigma = np.diag(S)\n",
    "T = np.dot(U,Sigma)\n",
    "P = P_T.T\n",
    "\n",
    "print(\"T (Scores)\", T, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"P (Loadings)\", P, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"Sigma (Variance)\", S, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn PCA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "T = pca.fit_transform(X)\n",
    "P = pca.components_.T\n",
    "latent = pca.explained_variance_\n",
    "explained = pca.explained_variance_ratio_\n",
    "S = pca.singular_values_\n",
    "Sigma = np.diag(S)\n",
    "\n",
    "print(\"T (Scores)\", T, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"P (Loadings)\", P, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"Sigma (Variance)\", S, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe that we can compute the explained variance ratios using the singular values returned from the PCA calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_2 = (S ** 2) / 4\n",
    "explained_variance_ratio_2 = (explained_variance_2 / explained_variance_2.sum())\n",
    "print(explained_variance_ratio_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue decomposition PCA implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the latent variable directions (the loading vectors) were oriented so that the variance of the\n",
    "scores in that direction were maximal. We can cast this as an optimization problem. For the first\n",
    "component:  \n",
    "$$max (\\phi)=\\mathbf{t_1^\\top}\\mathbf{t_1}=\\mathbf{p_1^\\top} \\mathbf{X^\\top}\\mathbf{Xp_1}$$\n",
    "such that $\\mathbf{p_1^\\top p_1}=1$.\n",
    "\n",
    "This is equivalent to $$max (\\phi)=\\mathbf{p_1^\\top} \\mathbf{X^\\top Xp_1}-\\lambda(\\mathbf{p_1^\\top}\\mathbf{p_1}-1)$$ \n",
    "\n",
    "because we can move the constraint into the objective function with a Lagrange multiplier, $\\lambda$. The maximum value must occur when the partial derivatives with respect to $\\mathbf{p_1}$, \n",
    "\n",
    "our search variable, are zero: $$\\frac{\\partial \\phi}{\\partial p_1}= \\frac{\\partial (\\mathbf{p_1^\\top X^\\top Xp_1}-\\lambda(\\mathbf{p}_{1}^{\\top}\\mathbf{p}_{1}-1))}{\\partial \\mathbf{p}_1}=0$$\n",
    "\n",
    "$$2\\mathbf{X^\\top X p_1}-2\\lambda_1\\mathbf{p_1}=0$$\n",
    "\n",
    "$$(\\mathbf{X^\\top X}-\\lambda_1\\mathbf{I})\\mathbf{p_1}=0$$\n",
    "\n",
    "$$\\mathbf{X^\\top Xp_1}=\\lambda_{1}\\mathbf{p_1}$$\n",
    "\n",
    "which is just the eigenvalue equation, indicating that $\\mathbf{p_1}$ is the eigenvector of $\\mathbf{X^\\top X}$ and $\\lambda_1$ is the eigenvalue. One can show that $\\lambda_1=\\mathbf{t_1^\\top t_1}$, which is proportional to the variance of the first component. In a similar manner we can calculate the second eigenvalue, but this time we add the additional constraint that $\\mathbf{p}_1 \\perp \\mathbf{p}_2$. Writing out this objective function and taking partial derivatives leads to showing that \n",
    "\n",
    "$$\\mathbf{X^\\top Xp_2} = \\lambda_2 \\mathbf{p_2}.$$\n",
    "\n",
    "From this we learn that:\n",
    "* The loadings are the eigenvectors of $\\mathbf{X^\\top X}$.\n",
    "* Sorting the eigenvalues in order from largest to smallest gives the order of the corresponding eigenvectors, the loadings.\n",
    "* We know from the theory of eigenvalues that if there are distinct eigenvalues, then their eigenvectors are linearly independent (orthogonal).\n",
    "* We also know the eigenvalues of $\\mathbf{X^\\top X}$ must be real values and positive; this matches with the interpretation that the eigenvalues are proportional to the variance of each score vector.\n",
    "* Also, the sum of the eigenvalues must add up to sum of the diagonal entries of $\\mathbf{X^\\top X}$, which represents of the total variance of the $\\mathbf{X}$ matrix, if all eigenvectors are extracted. So plotting the eigenvalues is equivalent to showing the proportion of variance explained in X by each component. This is not necessarily a good way to judge the number of components to use, but it is a rough guide: use a Pareto plot of the eigenvalues (though in the context of eigenvalue problems, this plot is called a scree plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg as la\n",
    "\n",
    "\n",
    "cov = np.cov(X, rowvar=False)\n",
    "evals, P = la.eigh(cov)\n",
    "idx = np.argsort(evals)[::-1]\n",
    "P = P[:, idx]\n",
    "evals = evals[idx]\n",
    "T = np.dot(X, P) \n",
    "Sigma = la.norm(T, axis=0)\n",
    "\n",
    "print(\"T (Scores)\", T, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"P (Loadings)\", P, sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"Sigma (Variance)\", S, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Test if the loading vectors are orthogonal and orthonormal or not  \n",
    "Implement the functions `is_orthogonal` and `is_orthonormal`, which both return a boolean value.  \n",
    "*Hint*: We can tolerate a small amount of error. The functions `np.allclose` and `np.eye` can be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_orthogonal(matrix: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the matrix is orthogonal.\n",
    "\n",
    "    Returns:\n",
    "        _ (bool): Whether or not the column vectors are orthogonal.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the function here\n",
    "    \n",
    "    \n",
    "orthogonal_P = is_orthogonal(P)\n",
    "assert isinstance(orthogonal_P, bool), \"You need to implement the function\"\n",
    "result_string = \"orthogonal\" if orthogonal_P else \"not orthogonal\"\n",
    "print(f\"The loading vectors are {result_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_orthonormal(matrix: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the matrix is orthonormal.\n",
    "\n",
    "    Returns:\n",
    "        _ (bool): Whether or not the column vectors are orthonormal.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the function here\n",
    "    # Hint: The function 'is_orthogonal' can be helpful here as well\n",
    "    \n",
    "    \n",
    "orthonormal_P = is_orthonormal(P)\n",
    "assert isinstance(orthonormal_P, bool), \"You need to implement the function\"\n",
    "result_string = \"orthonormal\" if orthonormal_P else \"not orthonormal\"\n",
    "print(f\"The loading vectors are {result_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Test if the scores vectors are orthogonal and orthonormal or not  \n",
    "*Hint*: Code is made to be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "orthogonal_T = is_orthogonal(T)\n",
    "assert isinstance(orthogonal_T, bool)\n",
    "result_string = \"orthogonal\" if orthogonal_T else \"not orthogonal\"\n",
    "print(f\"The scores vectors are {result_string}\")\n",
    "\n",
    "orthonormal_T = is_orthonormal(T)\n",
    "assert isinstance(orthonormal_T, bool)\n",
    "result_string = \"orthonormal\" if orthonormal_T else \"not orthonormal\"\n",
    "print(f\"The scores vectors are {result_string}\")## Task 3: Add more columns to the original data matrix by: \n",
    "* Make some of the columns to be the linear combination of others\n",
    "* Duplicate some columns\n",
    "* Add noise as some columns \n",
    "* Add a few columns of categorical values\n",
    "\n",
    "Then apply PCA to the dataset and report your findings here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler # Choose one\n",
    "import matplotlib.pyplot as plt # Maybe use to visualize something\n",
    "\n",
    "\n",
    "# TODO: Implement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
